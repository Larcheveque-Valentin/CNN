{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Aug 15 14:49:58 2022\n",
    "\n",
    "@author: Sidi Wu and Cédric Beaulac\n",
    "\n",
    "Functional autoencoder implementation\n",
    "\"\"\"\n",
    "\n",
    "# Import modules\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "#matplotlib.use('TkAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "# import skfda as fda\n",
    "# from skfda import representation as representation\n",
    "# from skfda.exploratory.visualization import FPCAPlot\n",
    "# # from skfda.exploratory.visualization import FPCAPlot\n",
    "# # from skfda.preprocessing.dim_reduction import FPCA\n",
    "# # from skfda.representation.basis import BSpline, Fourier, Monomial\n",
    "import scipy\n",
    "from scipy.interpolate import BSpline\n",
    "import os\n",
    "import ignite\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "from random import seed\n",
    "from scipy import stats\n",
    "import statistics\n",
    "from statistics import stdev\n",
    "import skfda\n",
    "\n",
    "import skfda\n",
    "from skfda import FDataGrid as fd\n",
    "from skfda.representation.basis import BSpline as B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       ...\n",
      "       'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173',\n",
      "       'V174'],\n",
      "      dtype='object', length=174)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(520, 174)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(\"C:/Users/Utilisateur/Documents/Master SSD/STAGE RECHERCHE/FDA_python/Functionnal_classifier\")\n",
    "# Lire le fichier CSV\n",
    "sofa = pd.read_csv('sofa.csv')\n",
    "\n",
    "# Attribuer des noms de colonnes personnalisés\n",
    "\n",
    "\n",
    "# Afficher les noms de colonnes\n",
    "print(sofa.columns)\n",
    "grid_points=np.arange(173)\n",
    "sofa.values.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([390, 1, 40]), torch.Size([390, 1, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_X=torch.from_numpy(sofa.values).float().cuda()\n",
    "\n",
    "y,x=Y_X[:,0].unsqueeze(1).unsqueeze(2).long(),Y_X[:,1:41].reshape(520,1,40)\n",
    "x_train,x_test,y_train,y_test=sklearn.model_selection.train_test_split(x,y,shuffle=True)\n",
    "x_train.shape,y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 3)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 3)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCNN,self).__init__()\n",
    "        self.convlayer1=nn.Sequential(\n",
    "\n",
    "            nn.Conv1d(1,128,kernel_size=7,stride=1,padding=0,dilation=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.18),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.convlayer2=nn.Sequential(\n",
    "\n",
    "            nn.Conv1d(128,64,kernel_size=4,stride=1,padding=0,dilation=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.18),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.convlayer3=nn.Sequential(\n",
    "\n",
    "            nn.Conv1d(64,32,kernel_size=7,stride=1,padding=0,dilation=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(0.18),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc_block=nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(25*32,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.18),\n",
    "        \n",
    "            nn.Linear(512,2),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        Conv_out=self.convlayer1(x)\n",
    "        Conv_out2=self.convlayer2(Conv_out)\n",
    "        Conv_out3=self.convlayer3(Conv_out2)\n",
    "        n_conv_out=Conv_out3.shape[1]\n",
    "        Lin_out=self.fc_block(Conv_out3)\n",
    "        return Lin_out.float().unsqueeze_(2).unsqueeze_(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([390, 1, 40]), torch.Size([390, 32, 25])]"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_conv_out=25\n",
    "CNN=FCNN().cuda().apply(weights_init_normal)\n",
    "Conv_out=CNN.convlayer3(CNN.convlayer2(CNN.convlayer1(x_train)))\n",
    "[x_train.shape,Conv_out.shape]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_adam=0.001\n",
    "lr_sgd=0.001\n",
    "batch_size=78\n",
    "betas=[0.45,0.98]\n",
    "# optimizer=optim.Adam(CNN.parameters(),lr=lr_adam,betas=betas)\n",
    "optimizer=optim.SGD(CNN.parameters(),lr=lr_sgd)\n",
    "loss_function=nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "def train(n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss=torch.tensor(0).cuda().long()\n",
    "        \n",
    "        for i in range(int(len(x_train)/batch_size)):\n",
    "            functions_train=x_train[batch_size*i:batch_size*(1+i),:,:]\n",
    "            labels_train=y_train[batch_size*i:batch_size*(1+i)]\n",
    "            optimizer.zero_grad()\n",
    "            output=CNN(functions_train)\n",
    "            loss=loss_function(input=output,target=labels_train)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss+=loss.long()\n",
    "        return train_loss,loss \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0, device='cuda:0'), tensor(0.1249, device='cuda:0', grad_fn=<NllLoss2DBackward0>))\n",
      "Précision moyenne = 82.30769 %\n"
     ]
    }
   ],
   "source": [
    "print(train(10))\n",
    "print(\"Précision moyenne =\",((torch.sum(torch.argmax(CNN(x_test),dim=1)==y_test)/x_test.shape[0])*100).detach().cpu().numpy(),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
