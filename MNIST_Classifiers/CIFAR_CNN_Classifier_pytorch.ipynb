{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## library\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd.variable import Variable\n",
    "from torch import nn, optim\n",
    "import argparse\n",
    "import matplotlib as plt \n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "import visdom\n",
    "import numpy as np\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Subset\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "##Classifier_cifar pytorch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim=100\n",
    "batch_size=60\n",
    "num_img_channel=3\n",
    "img_size=32\n",
    "sample_interval=64          \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_cifar(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier_cifar,self).__init__()\n",
    "\n",
    "        \n",
    "        \n",
    "        self.conv_layer1=nn.Sequential(\n",
    "            nn.Conv2d(num_img_channel,3,kernel_size=[7,7],stride=1,padding=2,bias =False,dilation=1),\n",
    "        \n",
    "            \n",
    "            nn.LeakyReLU(0.18,inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv_layer2=nn.Sequential(\n",
    "            nn.Conv2d(3,16,kernel_size=[6,6],stride=1,padding=0,bias =False,dilation=1,),\n",
    "        \n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.18,inplace=True),\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "        self.fc_block1= nn.Sequential(\n",
    "            \n",
    "            nn.Linear(16*25*25,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.18,inplace=True),\n",
    "            nn.Linear(512,10)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,img):\n",
    "        layer1_out=self.conv_layer1(img)\n",
    "        layer2_out=self.conv_layer2(layer1_out)\n",
    "        conv_out=layer2_out.view(img.size(0),16*25*25)\n",
    "        l2_value = self.fc_block1(conv_out)\n",
    "        l2_value2 = l2_value.unsqueeze_(dim=2).unsqueeze_(dim=3)\n",
    "        return l2_value2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions des ensembles d'entraînement :\n",
      "x_train: torch.Size([50000, 32, 32, 3])\n",
      "y_train: torch.Size([50000, 1])\n",
      "Dimensions des ensembles de test :\n",
      "x_test: torch.Size([10000, 32, 32, 3])\n",
      "y_test: torch.Size([10000, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "# Téléchargement des données CIFAR-10\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train_vis,x_test_vis=(x_train,x_test)\n",
    "(x_train, y_train), (x_test, y_test)= (torch.tensor(x_train),torch.tensor(y_train).cuda()), (torch.tensor(x_test),torch.tensor( y_test).cuda())\n",
    "# Affichage des dimensions des ensembles d'entraînement et de test\n",
    "print('Dimensions des ensembles d\\'entraînement :')\n",
    "print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('Dimensions des ensembles de test :')\n",
    "print('x_test:', x_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "x_train=x_train.permute(0,3,1,2).cuda().float()\n",
    "x_test=x_test.permute(0,3,1,2).cuda().float()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier_cifar=Classifier_cifar()\n",
    "classifier_cifar=classifier_cifar.cuda()\n",
    "\n",
    "classifier_cifar.apply(weights_init_normal)\n",
    "def Class_out(K):\n",
    "    if classifier_cifar(x_test)[K].argmax()==0:\n",
    "        print(\"C'est un Avion!\")\n",
    "    if classifier_cifar(x_test)[K].argmax()==1:\n",
    "        print(\"C'est une Voiture!\")\n",
    "    if classifier_cifar(x_test)[K].argmax()==2:\n",
    "        print(\"C'est un Oiseau!\")\n",
    "    if classifier_cifar(x_test)[K].argmax()==3:\n",
    "        print(\"C'est un chat!\")\n",
    "    if classifier_cifar(x_test)[K].argmax()==4:\n",
    "        print(\"C'est un Cerf!\")\n",
    "    if classifier_cifar(x_test)[K].argmax()==5:\n",
    "        print(\"C'est un Chien!\")\n",
    "    if classifier_cifar(x_test)[K].argmax()==6:\n",
    "        print(\"C'est une Grenouille!\")\n",
    "    if classifier_cifar(x_test)[K].argmax()==7:\n",
    "        print(\"C'est un Cheval!\")\n",
    "    if classifier_cifar(x_test)[K].argmax()==8:\n",
    "        print(\"C'est un Bateau!\")\n",
    "    if classifier_cifar(x_test)[K].argmax()==9:\n",
    "        print(\"C'est un Camion!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 3, 30, 30]), torch.Size([1, 16, 25, 25])]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Image_conv1=classifier_cifar.conv_layer1(x_test[0,:,:,:].unsqueeze(0))\n",
    "Image_conv2=classifier_cifar.conv_layer2(Image_conv1)\n",
    "\n",
    "[Image_conv1.shape,Image_conv2.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 10, 1, 1])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_cifar(x_test[:13,:,:,:]).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(np.uint8(x_test_vis[1001]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=random.randint(0,10000)\n",
    "Image_vis=x_test[K,:,:,:].unsqueeze(0)\n",
    "Image_vis_conv1=classifier_cifar.conv_layer1(Image_vis)\n",
    "Image_vis_conv2=classifier_cifar.conv_layer2(Image_vis_conv1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 16), |u1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Utilisateur\\anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   2952\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2953\u001b[1;33m             \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2954\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ((1, 1, 16), '|u1')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17072\\2969861863.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Créez une image à partir du tableau NumPy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage_vis_conv2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Utilisateur\\anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   2953\u001b[0m             \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2954\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2955\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot handle this data type: %s, %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtypekey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2956\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2957\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 16), |u1"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "Image_vis_conv1=classifier_cifar.conv_layer1(Image_vis)\n",
    "Image_vis_conv2=classifier_cifar.conv_layer2(Image_vis_conv1)\n",
    "\n",
    "Image_vis_conv1=Image_vis_conv1.cpu().detach().numpy()\n",
    "Image_vis_conv2=Image_vis_conv2.cpu().detach().numpy()\n",
    "Image_vis_conv1=np.transpose(Image_vis_conv1,(0,2,3,1))\n",
    "Image_vis_conv2=np.transpose(Image_vis_conv2,(0,2,3,1))\n",
    "\n",
    "Image_vis_conv1=Image_vis_conv1.squeeze(0)\n",
    "Image_vis_conv2=Image_vis_conv2.squeeze(0)\n",
    "\n",
    "# Créez une image à partir du tableau NumPy\n",
    "Image.fromarray(np.uint8(Image_vis_conv2))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "b1=0.5\n",
    "b2=0.999\n",
    "optimizer=optim.Adam(classifier_cifar.parameters(),lr=lr,betas=(b1,b2))\n",
    "loss_C=nn.CrossEntropyLoss()\n",
    "\n",
    "indice=np.arange(50000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(n_epochs):\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        print('Epoch {}'.format(epoch))\n",
    "        shuffled_indices=np.random.permutation(indice)\n",
    "        x_train_shuffled=x_train[shuffled_indices,:,:,:]\n",
    "        y_train_shuffled=y_train[shuffled_indices]\n",
    "        for i in range(int(len(x_train)/batch_size)):\n",
    "            imgs= x_train_shuffled[i*batch_size:(i+1)*batch_size,:,:,:]\n",
    "            \n",
    "        \n",
    "            labels = y_train_shuffled[i*batch_size:(i+1)*batch_size]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = classifier_cifar(imgs)\n",
    "\n",
    "            loss = loss_C(output, labels.unsqueeze(1).long() )\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.45s/it]\n"
     ]
    }
   ],
   "source": [
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3]],\n",
       "\n",
       "        [[9]],\n",
       "\n",
       "        [[8]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[3]],\n",
       "\n",
       "        [[5]],\n",
       "\n",
       "        [[7]]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(classifier_cifar(x_test),dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=random.randint(0,10000)\n",
    "Image.fromarray(np.uint8(x_test_vis[K]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAIAAAC0Ujn1AAAEJ0lEQVR4nH2W0ZLdNg5ETwOkJF9nq/L/3+nEvhKJ3gdKM5ktZ/UISUCj2Q1QAAhMg8xW4hoKb38QU++T8yIgwGIC5v89IRlsBwHP5xUR2tJpSB3f4ZfPtw2RKLBBoH9JuuI2siBEPEFRFRRyJn5xnZzDzSiZweWF6cbyvxk/6hnpZiE+Y85xtZPMOpr462/eRIW2hgsZgU39FvUnS2FjQRFoxWXwjHFGcW2X68KJD88CIDGowF8p+UK9ABc2QJO1MMPEDeabObGlPNwKXwa5YxNGiQvq7vqhmBue73BBs3zDBqKyzzFvCDkYF8LIKhkI1GCuJgU4TIhCE5uAeio1iEWJSYgMTxPfFd+YP1woNvXmgdfJCCogaCJOajB3l+USYYf1SQjOtCeTcrEHiG+d2Wva2+Y9tQ/ORmJJsxiBN9J2WcNdXMPDQrKQb3Ya2IIgKIgpUPrSSfWN7YWzrnftob2ZdwHRcMnl6romv651VAZhLFKU3XB4Lr2AmX+KLr+DV0wx/yI3IlD6J3I4BWFNlEqR4IEdHQomt61UNHDZ4EBCzE6fF1V2NfrfvE/1ZB6qgvIFo6lOtRb9KJcvKFGPeQLVRNSnJhshJz34Q835nwP+XG1+KNmBO2wigib2jQ03mbTvb5LQgt5godUSwLzMYW18U/ycXFiOR8ICCk/B4QrmL4ybtE89tq5H2loVUkEUNpPO9ytavX76x5W0wTAEjezeGvXDw0oJPGyj3PCpedcH0fLWOMIUYpYmDN5RMAfUZARHLL/2TUbToWCYsyKhgRLn7RM9Iqli8RHLC6lM7zid0OnLfxvdbvB6tT0Cgu+w22I/eKm2ZclnPEFG/mOsJAh1stEpCnzwDfYt6Ou0ff9gaNu+Hxxbfyk3GsZ+5tTn7LoJCRR4MidXBkIKCPazbk9ZdYEJQZ3vWXv3Zs8T61bCze1TIgBFGE2vcZY9m8BvfjUJXnUPojUiA6JA53Vd42TZ/zfrZ60OIgMwLaKJRGGh4FCi783R1r6jbw5xVNtuK4CgPwQ82D94EVJbRmrKuJesFHoBHDi1MujV94RuIsnbIW58gpY+80dg8BQt3Yep+x+7PO7K03Qym6bfHVJsO5kGLAxp4QDhr8cItAjV2jSMVdZwEmhABFK2UWWiR0bIjKmwba39BMUSij9Si+7MWdMPPXG/zcwWbrAL+zSEiaiqun6uluUvc+hTIhKhbLHXuRaPOEyf0YhQ2/YKUaB0whtmgXkbgToO4nc3Ht9cR9WloG2AqQBKqmyjh2OCrXYv2EnO09a9xmb9JqvW5DcRV1ElhhJyGRWhcBvzPvaCsWTDfCMaCtoAP2z8Q9N++mhvkymCsBlhgSPwOWNWZNQletMA3febTE3XV4r9jL2vTya9qfUlhBVahDUdx9KQ/bH/0Rff/fst8L+8GXsrQL/HSAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=30x30>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Image_vis=x_test[K,:,:,:].unsqueeze(0)\n",
    "Image_vis_conv1=classifier_cifar.conv_layer1(Image_vis)\n",
    "Image_vis_conv2=classifier_cifar.conv_layer2(Image_vis_conv1)\n",
    "\n",
    "Image_vis_conv1=Image_vis_conv1.cpu().detach().numpy()\n",
    "Image_vis_conv2=Image_vis_conv2.cpu().detach().numpy()\n",
    "\n",
    "Image_vis_conv1=np.transpose(Image_vis_conv1,(0,2,3,1))\n",
    "Image_vis_conv2=np.transpose(Image_vis_conv2,(0,2,3,1))\n",
    "\n",
    "Image_vis_conv1=Image_vis_conv1.squeeze(0)\n",
    "Image_vis_conv2=Image_vis_conv2.squeeze(0)\n",
    "\n",
    "# Créez une image à partir du tableau NumPy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Image.fromarray(np.uint8(Image_vis_conv1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est un Oiseau!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Class_out(K)\n",
    "\n",
    "Image.fromarray(np.uint8(x_test_vis[K]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABAAAAAZCAAAAAAdl/F1AAAALElEQVR4nGNggABGRgYYQLBwCDGiM5lQRDH0MzEwIamDK8OtAY/dhISHLQAAHqIAF+kQ890AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=16x25>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(np.uint8(Image_vis_conv2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision moyenne= 59.25 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy=(torch.sum(torch.argmax(classifier_cifar(x_test),dim=1).squeeze(1)==y_test))*100/10000\n",
    "print('Précision moyenne=', np.array((accuracy.float().cpu())),\"%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
